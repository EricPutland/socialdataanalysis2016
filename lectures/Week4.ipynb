{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "## Overview\n",
    "\n",
    "This week is all about regression. Linear regression, multiple regression, and logistic regression. It is extremely lucky that we're learning about regression because this is the tool we'll need to counter a new threat facing SF. It turns out that _there is a **serial flasher** on the loose in the city_, exposing his naked body to the unsuspecting citizens at all hours of the day and night. The flasher has earned the nickname _the Red Baron_ because of his baboon-level red butt. \n",
    "\n",
    "But I was so excited by the optional KNN exercise from last week, that we'll include that as well. \n",
    "\n",
    "Here's a summary of the program:\n",
    "\n",
    " * Sune will address the class and focus on two things: Signing in to `peergrade.io` and how we like that the book is \"from scratch\", but that we generally recommend that you guys use tools out there (such as `scikit-learn`) when you solve the exercises.\n",
    " * As a call-back to last time, we start by explore KNN to study the patterns of Drugs, Prostitution, and Drunk driving across SF. So if you did the optional exercise last time, this one will be easy\n",
    " * \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: KNN revisited\n",
    "\n",
    "> _Exercise_: K-nearest-neighbors map.\n",
    "> \n",
    "> The goal of this exercise is to create a useful real-world version of the example on pp153 in DSFS. We know from last week's exercises that the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map. \n",
    "> \n",
    "> * Begin by using `geoplotlib` to plot all incidents of the three crime types on their own map using [`geoplotlib.kde()`](https://github.com/andrea-cuttone/geoplotlib/blob/master/examples/kde.py). This will give you an idea of how the varioius crimes are distributed across the city.\n",
    "> * Next, it's time to set up your model based on the actual data. You can use the code supplied in the book or try out `scikit-learn`'s [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). If you end up using the latter (recommended), you may want to check out [this example](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html) to get a sense of the usage.\n",
    ">   - You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. **But** do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset). Since the algorithm classifies each point according to it's neighbors, what could a consequence of this imbalance in the number of examples from each class mean for your map?\n",
    ">   - You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category. How do you expect that will change the KNN result? In which situations is the balanced map useful - and when is the map that data in proportion to occurrences useful? Choose which map you will work on in the following. \n",
    "> * Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50 \\times 50$ and $100 \\times 100$ points. I recommend plotting using `geoplotlib.dot()`. \n",
    "> * Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its 5, 10, and 30 nearest neighbors. Describe what happens to the map as you increase the number of neighbors, `K`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression (DSFS Chapter 14)\n",
    "\n",
    "> _Reading_: Work through DSFS Chapter 14. \n",
    "\n",
    "Now it's time to watch a little Ole-video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"338\"\n",
       "            src=\"https://www.youtube.com/embed/_XK8JcpKsBw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x10621b150>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole discusses Linear Regression\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"_XK8JcpKsBw\",width=600, height=338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercises_: Just a few questions to check that you've read the material.\n",
    "> \n",
    "> * What's the point of the error term $\\epsilon_i$? (That one wasn't around when you learned how to figure out the slope and $y$ intercept of straight lines in high school).\n",
    "> * Why do you think Joel figures out how to find $\\alpha$ and $\\beta$ using gradient descent when we already know how to find the best values analytically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building an understanding of Multiple Regression and Logistic Regression\n",
    "\n",
    "> _Reading_: Start by reading DSFS Chapter 15.\n",
    "\n",
    "And let's watch another video to stimulate the visual part of your brain + get an expert's (Ole's) opinon on what's most important about Multiple regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"338\"\n",
       "            src=\"https://www.youtube.com/embed/lLWJbCFoB30\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1061fd350>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole explains Multiple Regression\n",
    "YouTubeVideo(\"lLWJbCFoB30\",width=600, height=338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercises_: Now some questions for the text. Answer the following questions in your own words\n",
    ">   \n",
    "> * It's a requirement that the inputs of the model are linearly independent. Why is that important? Illustrate what can go wrong using a simple example.\n",
    "> * What is _bootstrap_? When is it useful?\n",
    "> * What is _regularization_? In what cases is reqularization important? Illustrate what can go wrong using a simple example.\n",
    "\n",
    "---\n",
    "\n",
    "> _Reading_: Go over DSFS Chapter 16 on Logistic regression\n",
    "\n",
    "And then watch the corresponding video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"338\"\n",
       "            src=\"https://www.youtube.com/embed/CuJQazbT6d0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1061fdc10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ole explains Logistic Regression\n",
    "YouTubeVideo(\"CuJQazbT6d0\",width=600, height=338)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Use regression models to help SFPD capture the dreaded _Red Baron_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may remember from the _overview_ that we've found a serial flasher on the loose in SF. He has earned the nickname ***The Red Baron*** because of his characteristic red butt, which is as red as a baboon's behind.\n",
    "\n",
    "There's a new dataset (internally called _the x-files_), which is too classified to be put on the SFOpenData webpage. This dataset contains a new crime type called `INDECENT EXPOSURE` and you can download it [**here**](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/x-files.csv). (_Psst: let's get real for a second ... in case you were in doubt, this dataset has been made up by your loving teaching staff_). The dataset contains all indecent exposure cases (SF has a lot of those), not just those caused by the Red baron. Some of the cases, however, carry the label `Red Baron` (whenever a victim has spotted his highly conspicuous Red Rump).\n",
    "\n",
    "In this dataset, due to a data error, we don't have the districts, but we have GPS locations.\n",
    "\n",
    "> _Exercise_: Your first exercise is localize where the Red Baron. The idea is to use the crimes labeled `INDECENT EXPOSURE` figure out where the Red Baron tends to strike. Each case has a `Resolution` field: in case of the Red Baron was recognized, it is indicated in this field, otherwise it shows `NONE`.\n",
    "> Use **logistic regression** to predict if we have the Red Baron or not provided we know the longitude and latitude of the crime.\n",
    "> \n",
    "> * We will train our data to estimate the regression parameters and then validate the results by calculating performance measures on another chunk of the data. To achieve this, train your model using 80% of the data and then calculate regression performance on the rest 20%. Calculate accuracy, precision and recall to see how good our model is! (Note that the Red Baron appears only in later years, so avoid selecting the first 80% of the data, but pick random cases instead.). \n",
    "> \n",
    ">   Our input variables are the longitude and latitude of the cases, and output variables are whether the crime was committed by the Red Baron or not. Note that GPS coordinates have very low variance which can cause some trouble in the optimization. Try to rescale your training and validation sets into between 0 and 1 before performing the regression.\n",
    "> * For the regression, you can use the code in the DSFS book (Chapters 14-16) for the regression, or the corresponding [logistic regression method](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) in the scikit-learn module. Note that you don't need to write all code for yourself, for example, scipy provides a convenience method for high performance optimization: [scipy.optimize.minimize](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize).\n",
    "> * When you have the regression parameters, plot the cases along with the decision boundary (you don't need to worry about scaling the location variables, you can use the rescaled inputs)! Which district is that?\n",
    "> * (Optional) Estimate how the regression parameters deviate by using bootstrapping. In this case, you can pick a small chunk of the data (500 cases for each output), run the regression on half of the selected data 100 times and then calculate the mean and standard deviation of the fit parameters.\n",
    "\n",
    "Just to give you guys a sense of what you should be expecting, our performance values were:\n",
    "\n",
    "* `accuracy`:  0.89\n",
    "* `precision`: 0.78\n",
    "* `recall`:    0.94\n",
    "\n",
    "But these are just guidelines and your results might be different.\n",
    "\n",
    "And the rescaled coordinates and decision boundary looks something like this:\n",
    "\n",
    "![the decision boundary](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/b.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a high stress situation, and the heat is coming down from the public officials to stop this red-assed scurge on the beautiful city of San Francisco. Right as the police force is beginning to disintegrate and morale is dwindeling, Police chief Suneman brings forward the idea that the Red Baron may not as random as previously assumed.\n",
    "\n",
    "Chief Suneman's idea is that the Red Baron might pick the time of his attacks according to a pattern that we can detect using the powers of data science (with great power comes great responsibility).\n",
    "\n",
    "If he's right, we can identify the time of the next attack, which will help us end this insanity once and for all. Well, let's see if he is right!\n",
    "\n",
    "> Exercise: Predict the Red Baron's pattern.\n",
    "> \n",
    "> \n",
    "> * Start from all cases having `Red Baron` in the resolution field and use the day of the week to predict the hour of the day when he is attacking, e.g. use **linear regression** to infer the hour of the day based on the weekday! Again, take 4/5 of the data for training and then calculate goodness of fit using $R^2$ on the rest 1/5. Don't forget to rescale your input variables! (Note 1: My goodness of fit after using the weekdays is only around 0.618). (Note 2: For multivariate regression, as always you can simply re-use the code in the DSFS book (Chapters 14-15) or scikit-learn).\n",
    "> * Now, add the crime year as well to the input variables! Did the goodness of fit improve? (Note: Mine did to 0.809)\n",
    "> * It is still very low. Inspired by a movie he once watched, Chief Suneman yells: \"Let's add the longitude of the crimes as well!\" Is your prediction getting better? (It should, to around 0.993)\n",
    "> * Very nice! Why not add latitude as well? What do you find now?\n",
    "> * (Optional) The gruff Deputy Chief Winther suspects that latitude has nothing to do with the time of crimes. Check this by estimating parameter deviations using the bootstrap method. What is the deviation of the coefficient for the latitude? Again, for bootstrap, you can use a small bit of the data (e.g., 500 crimes) to have a faster running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
